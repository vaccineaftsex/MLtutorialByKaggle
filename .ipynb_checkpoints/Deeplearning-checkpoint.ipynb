{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convolutions\n",
    "    * more visual patterns can be captured by large convolutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose image to work with\n",
    "from os.path import join\n",
    "\n",
    "image_dir = '../input/dog-breed-identification/train/'\n",
    "img_paths = [join(image_dir, filename) for filename in \n",
    "                           ['0246f44bb123ce3f91c939861eb97fb7.jpg',\n",
    "                            '84728e78632c0910a69d33f82e62638c.jpg',\n",
    "                            '8825e914555803f4c67b26593c9d5aff.jpg',\n",
    "                            '91a5e8db15bccfb6cfa2df5e8b95ec03.jpg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Read and Prep Images for Modeling\n",
    "import numpy as np\n",
    "from tensorflow.python.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "def read_and_prep_images(img_paths, img_height=image_size, img_width=image_size):\n",
    "    imgs = [load_img(img_path, target_size=(img_height, img_width)) for img_path in img_paths]\n",
    "    img_array = np.array([img_to_array(img) for img in imgs])\n",
    "    output = preprocess_input(img_array)\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Model with Pre-Trained Weights File. Make Predictions\n",
    "from tensorflow.python.keras.applications import ResNet50\n",
    "\n",
    "my_model = ResNet50(weights='../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels.h5')\n",
    "test_data = read_and_prep_images(img_paths)\n",
    "preds = my_model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise predictions\n",
    "from learntools.deep_learning.decode_predictions import decode_predictions\n",
    "from IPython.display import Image, display\n",
    "\n",
    "most_likely_labels = decode_predictions(preds, top=3, class_list_path='../input/resnet50/imagenet_class_index.json')\n",
    "\n",
    "for i, img_path in enumerate(img_paths):\n",
    "    display(Image(img_path))\n",
    "    print(most_likely_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "   * use the model of pretrained model\n",
    "   * only train the last layer -> new prediction layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model\n",
    "from tensorflow.python.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "num_classes = 2\n",
    "resnet_weights_path = '../input/resnet50/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'  # doesnt include the weight of the last layer\n",
    "\n",
    "my_new_model = Sequential()\n",
    "#exclude the layer that makes prediction; collapse the last layer into a 1D tensor by using average across channels\n",
    "my_new_model.add(ResNet50(include_top=False, pooling='avg', weights=resnet_weights_path))  \n",
    "# add the new prediction layer, specify the number of nodes,\n",
    "my_new_model.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "# Say not to train first layer (ResNet) model. It is already trained\n",
    "my_new_model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile models\n",
    "my_new_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "from tensorflow.python.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = 224\n",
    "data_generator = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "        '../input/urban-and-rural-photos/rural_and_urban_photos/train',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=24,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "        '../input/urban-and-rural-photos/rural_and_urban_photos/val',\n",
    "        target_size=(image_size, image_size),\n",
    "        class_mode='categorical')\n",
    "\n",
    "my_new_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=3,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications.resnet50 import preprocess_input\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = 224\n",
    "\n",
    "data_generator_with_aug = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   horizontal_flip=True,      # \"horizontal_flip\" randomly decide whether to flip the image horizontally\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2)\n",
    "\n",
    "train_generator = data_generator_with_aug.flow_from_directory(\n",
    "        '../input/urban-and-rural-photos/rural_and_urban_photos/train',\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=24,\n",
    "        class_mode='categorical')\n",
    "\n",
    "# validation generator will use an ImageDataGenerator without augmentation. \n",
    "# allows a straightforward comparison between different training procedures (e.g. training with augmentation and without it).\n",
    "data_generator_no_aug = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "validation_generator = data_generator_no_aug.flow_from_directory(\n",
    "        '../input/urban-and-rural-photos/rural_and_urban_photos/val',\n",
    "        target_size=(image_size, image_size),\n",
    "        class_mode='categorical')\n",
    "\n",
    "my_new_model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=3,\n",
    "        epochs=2,  # goes through each raw image file 2 times\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def data_prep(raw):\n",
    "    out_y = keras.utils.to_categorical(raw.label, num_classes)\n",
    "\n",
    "    num_images = raw.shape[0]\n",
    "    x_as_array = raw.values[:,1:]\n",
    "    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = x_shaped_array / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "train_file = \"../input/digit-recognizer/train.csv\"\n",
    "raw_data = pd.read_csv(train_file)\n",
    "\n",
    "x, y = data_prep(raw_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(3, 3),   # 20 filters\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_rows, img_cols, 1)))  # The first layer always requires that you specify the `input_shape`\n",
    "model.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 dropouts and strides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python import keras\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "def data_prep(raw):\n",
    "    out_y = keras.utils.to_categorical(raw.label, num_classes)\n",
    "\n",
    "    num_images = raw.shape[0]\n",
    "    x_as_array = raw.values[:,1:]\n",
    "    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n",
    "    out_x = x_shaped_array / 255\n",
    "    return out_x, out_y\n",
    "\n",
    "train_size = 30000\n",
    "train_file = \"../input/digit-recognizer/train.csv\"\n",
    "raw_data = pd.read_csv(train_file)\n",
    "\n",
    "x, y = data_prep(raw_data)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(30, kernel_size=(3, 3),\n",
    "                 strides=2,\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_rows, img_cols, 1)))\n",
    "model.add(Dropout(0.5))                                                          # dropout layer\n",
    "model.add(Conv2D(30, kernel_size=(3, 3), strides=2, activation='relu'))          # strides = 2\n",
    "model.add(Dropout(0.5))                                                          # dropout layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x, y,\n",
    "          batch_size=128,\n",
    "          epochs=2,\n",
    "          validation_split = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
